{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea4db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from random import shuffle\n",
    "import pdb\n",
    "\n",
    "\n",
    "def get_files(cur_dir, regex):\n",
    "    out_files = []\n",
    "    for subdir,_,files in os.walk(cur_dir):\n",
    "        out_files.extend([ os.path.join(subdir, file)\n",
    "                           for file in files if re.search(regex, file)])\n",
    "    return out_files\n",
    "\n",
    "def get_file_prefix(filename):\n",
    "    return re.search('(.*\\\\\\\\[a-z0-9A-Z_-]*)_[0-9]*.jpg', filename).group(1)\n",
    "\n",
    "def all_directions_exist(prefix):\n",
    "    return (os.path.isfile(prefix + '_0.jpg') and\n",
    "            os.path.isfile(prefix + '_90.jpg') and\n",
    "            os.path.isfile(prefix + '_180.jpg') and\n",
    "            os.path.isfile(prefix + '_270.jpg'))\n",
    "\n",
    "def prefix_to_filenames(prefix):\n",
    "    return (prefix + '_0.jpg', prefix + '_90.jpg', prefix + '_180.jpg',\n",
    "            prefix + '_270.jpg')\n",
    "\n",
    "def read_grouped_filenames_and_labels(root):\n",
    "    labeled_filenames = []\n",
    "    all_labels = []\n",
    "    for dir in os.listdir(root):\n",
    "        cur_dir = os.path.join(root, dir)\n",
    "        if os.path.isdir(cur_dir):\n",
    "            label = dir\n",
    "            all_labels.append(label)\n",
    "            file_prefixes = [ get_file_prefix(file) for file in get_files(cur_dir, '_0.jpg') ]\n",
    "            cur_files = [ prefix_to_filenames(prefix) for prefix in file_prefixes\n",
    "                          if all_directions_exist(prefix) ]\n",
    "            labeled_filenames.extend([ (filenames, len(all_labels)-1)\n",
    "                                       for filenames in cur_files ])\n",
    "    files = [ tmp[0] for tmp in labeled_filenames ]\n",
    "    labels = [ tmp[1] for tmp in labeled_filenames ]\n",
    "\n",
    "    return files,labels, all_labels\n",
    "\n",
    "def train_val_split(files, labels):\n",
    "    frac = 0.1\n",
    "    zipped = list(zip(files, labels))\n",
    "    shuffle(zipped)\n",
    "    valsize = int(len(zipped)*frac)\n",
    "    val = zipped[:valsize]\n",
    "    train = zipped[valsize:]\n",
    "    train_files = [ e[0] for e in train ]\n",
    "    train_labels = [ e[1] for e in train ]\n",
    "    val_files = [ e[0] for e in val ]\n",
    "    val_labels = [ e[1] for e in val ]\n",
    "\n",
    "    return train_files, train_labels, val_files, val_labels\n",
    "\n",
    "\n",
    "\n",
    "# Tensorflow functions for mapping a dataset of filenames to the actual images they reference\n",
    "\n",
    "def parse_image(filename):\n",
    "    filecontents = tf.io.read_file(filename)\n",
    "    jpeg = tf.image.decode_jpeg(filecontents)\n",
    "    jpeg.set_shape([256, 256, 3])\n",
    "    return jpeg\n",
    "def normalize_image(img):\n",
    "    img = tf.cast(img, dtype=tf.float32)/255.0\n",
    "    img = img - tf.constant([ 0.485, 0.456, 0.406 ], shape=[1,1,3])\n",
    "    img = img/tf.constant([0.229,0.224,0.225], shape=[1,1,3])\n",
    "    return img\n",
    "\n",
    "\n",
    "def _color_jitter(img, b, c, s):\n",
    "    img = tf.image.random_brightness(img, max_delta = b)\n",
    "    img = tf.image.random_contrast(img, 1-c, 1+c)\n",
    "    img = tf.image.random_saturation(img, 1-s, 1+s)\n",
    "    return img\n",
    "\n",
    "def _lighting_noise(img, alphastd):\n",
    "    eigvec = tf.constant([[-0.5675,0.7192,0.4009 ],\n",
    "                          [-0.5808,-0.0045,-0.8140 ],\n",
    "                          [-0.5836,-0.6948,0.4203]])\n",
    "    eigval = tf.constant([ 0.2175, 0.0188, 0.0045 ])\n",
    "    alpha = tf.random_normal((), mean=0, stddev=alphastd)\n",
    "    imgnoise = tf.reshape(tf.reduce_sum(eigvec*eigval*alpha, 1), [1, 1, 3])\n",
    "    return img + imgnoise\n",
    "\n",
    "\n",
    "def grouped_streetview_dataset(files, labels, batch_size, augment = True, shuffle = True):\n",
    "    def augment_image(img):\n",
    "        flip = tf.greater(tf.random_uniform((), 0, 1), 0.5)\n",
    "\n",
    "        img = tf.cond(flip, lambda: img, lambda: img[:,::-1,:])\n",
    "        img = _color_jitter(img, 0.4, 0.4, 0.4)\n",
    "        img = _lighting_noise(img, 0.1)\n",
    "        return img\n",
    "    def augment_images(n,e,s,w):\n",
    "        return augment_image(n), augment_image(e), augment_image(s), augment_image(w)\n",
    "    def parse_images(files):\n",
    "        return (parse_image(files[0]), parse_image(files[1]),\n",
    "                parse_image(files[2]), parse_image(files[3]))\n",
    "    def normalize_images(n,e,s,w):\n",
    "        return (normalize_image(n), normalize_image(e),\n",
    "                normalize_image(s), normalize_image(w))\n",
    "    filename_dataset = tf.data.Dataset.from_tensor_slices(files)\n",
    "    d = filename_dataset.map(parse_images).prefetch(100)\n",
    "    daug = d.map(normalize_images).map(augment_images)\n",
    "    d = tf.data.Dataset.zip((filename_dataset, d, daug if augment else d.map(normalize_images),\n",
    "                             tf.data.Dataset.from_tensor_slices(labels))).prefetch(100)\n",
    "    if shuffle:\n",
    "        d = d.shuffle(4000)\n",
    "    d = d.batch(batch_size)\n",
    "    return d\n",
    "\n",
    "\n",
    "def streetview_dataset(files, labels, batch_size, augment = True, shuffle = True):\n",
    "    def augment_image(img):\n",
    "        flip = tf.greater(tf.random_uniform((), 0, 1), 0.5)\n",
    "\n",
    "        img = tf.cond(flip, lambda: img, lambda: img[:,::-1,:])\n",
    "        img = _color_jitter(img, 0.4, 0.4, 0.4)\n",
    "        img = _lighting_noise(img, 0.1)\n",
    "        return img\n",
    "\n",
    "    filename_dataset = tf.data.Dataset.from_tensor_slices(files)\n",
    "    d = filename_dataset.map(parse_image).prefetch(100)\n",
    "    daug = d.map(normalize_image).map(augment_image)\n",
    "    d = tf.data.Dataset.zip((d, daug if augment else d.map(normalize_image),\n",
    "                             tf.data.Dataset.from_tensor_slices(labels))).prefetch(100)\n",
    "    if shuffle:\n",
    "        d = d.shuffle(4000)\n",
    "    d = d.batch(batch_size)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712730d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
